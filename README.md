# ClassifierExperiment
Sentiment classification is a frequently used tool in businesses and an important topic in the realm of machine learning. Many industries use sentiment classification to improve their products and services via customer feedback forms. Given that different individuals have varying styles of writing, text processing is difficult as text are highly variable and ambiguous. Some people write with explicit meaning while some prefer writing more implicitly. The present study will investigate sentiment classifications from tweets. Tweets are dissimilar from feedback as they tend to be unfiltered and convey the raw emotions of their writer. They are also diverse in their content as users write about any topic, concerns, or interests they so desire. Feedback on the other hand, may contain individual biases and may not convey the full sentiment experienced by their writers. Feedback is also less variable in the types of verbs and nouns used as they usually concern over the same topic across all points of feedback. For example, a bank wishes to collect feedback relating to their online banking service, all feedback will be concerned on that given context and are hence less variable. Tweets are therefore more abstract in their nature compared to feedback and are likely to be more difficult to classify than feedback.
The present study aims to evaluate which base classifiers are most cost effective and produce the most meaningful accuracies for sentiment classification. We also aim to evaluate whether poor-performing classifiers can be improved through ensemble learning techniques such as bagging. The training dataset contains over 21000 instances of tweets from varying topics and concerns. Each labelled as having either, ‘positive’, ‘neutral’, or ‘negative sentiments. 
